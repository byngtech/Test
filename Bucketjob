#!/usr/bin/env python3
"""
File: copy_schemas_to_gcs.py
Description: Upload schema files to GCS bucket maintaining directory structure
"""

import os
import sys
import logging
from pathlib import Path
from datetime import datetime
from google.cloud import storage
from typing import List, Tuple

# Configure logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    datefmt='%Y-%m-%d %H:%M:%S'
)
logger = logging.getLogger(__name__)


class SchemaUploader:
    """Handle uploading schema files to GCS with directory structure preservation"""
    
    def __init__(self, bucket_name: str, gcp_project: str, base_folder: str = ""):
        """
        Initialize the uploader
        
        Args:
            bucket_name: GCS bucket name
            gcp_project: GCP project ID
            base_folder: Base folder path in GCS bucket (optional)
        """
        self.bucket_name = bucket_name
        self.gcp_project = gcp_project
        self.base_folder = base_folder.strip('/') if base_folder else ""
        
        # Initialize GCS client (uses Workload Identity automatically)
        self.storage_client = storage.Client(project=gcp_project)
        self.bucket = self.storage_client.bucket(bucket_name)
        
        logger.info(f"Initialized uploader for bucket: {bucket_name}")
        logger.info(f"GCP Project: {gcp_project}")
        logger.info(f"Base folder: {self.base_folder or '(root)'}")
    
    def find_schema_files(self, source_dir: str) -> List[Tuple[Path, str]]:
        """
        Find all files in source directory and compute their relative paths
        
        Args:
            source_dir: Source directory containing schema files
            
        Returns:
            List of tuples (file_path, relative_path)
        """
        source_path = Path(source_dir)
        
        if not source_path.exists():
            raise FileNotFoundError(f"Source directory not found: {source_dir}")
        
        files = []
        for file_path in source_path.rglob('*'):
            if file_path.is_file():
                # Compute relative path from source directory
                relative_path = file_path.relative_to(source_path)
                files.append((file_path, str(relative_path)))
        
        return files
    
    def upload_file(self, local_path: Path, gcs_path: str) -> bool:
        """
        Upload a single file to GCS
        
        Args:
            local_path: Local file path
            gcs_path: Destination path in GCS
            
        Returns:
            True if successful, False otherwise
        """
        try:
            blob = self.bucket.blob(gcs_path)
            blob.upload_from_filename(str(local_path))
            logger.info(f"✓ Uploaded: {local_path.name} -> gs://{self.bucket_name}/{gcs_path}")
            return True
        except Exception as e:
            logger.error(f"✗ Failed to upload {local_path}: {str(e)}")
            return False
    
    def upload_schemas(self, source_dir: str) -> Tuple[int, int]:
        """
        Upload all schema files maintaining directory structure
        
        Args:
            source_dir: Source directory containing schema files
            
        Returns:
            Tuple of (successful_uploads, failed_uploads)
        """
        logger.info("=" * 60)
        logger.info("Starting Schema Upload Process")
        logger.info("=" * 60)
        
        # Find all files
        files = self.find_schema_files(source_dir)
        total_files = len(files)
        
        if total_files == 0:
            logger.warning(f"No files found in {source_dir}")
            return 0, 0
        
        logger.info(f"Found {total_files} files to upload")
        logger.info("")
        
        # Display directory structure
        logger.info("Directory structure:")
        for _, relative_path in sorted(files):
            logger.info(f"  {relative_path}")
        logger.info("")
        
        # Upload files
        successful = 0
        failed = 0
        
        for local_path, relative_path in files:
            # Construct GCS path maintaining directory structure
            if self.base_folder:
                gcs_path = f"{self.base_folder}/{relative_path}"
            else:
                gcs_path = relative_path
            
            if self.upload_file(local_path, gcs_path):
                successful += 1
            else:
                failed += 1
        
        return successful, failed
    
    def verify_upload(self, source_dir: str) -> bool:
        """
        Verify uploaded files in GCS
        
        Args:
            source_dir: Source directory to compare against
            
        Returns:
            True if verification successful
        """
        logger.info("")
        logger.info("=" * 60)
        logger.info("Verifying Upload")
        logger.info("=" * 60)
        
        # Get local files
        local_files = {rel_path for _, rel_path in self.find_schema_files(source_dir)}
        
        # List GCS files
        prefix = f"{self.base_folder}/" if self.base_folder else ""
        blobs = list(self.bucket.list_blobs(prefix=prefix))
        
        gcs_files = set()
        logger.info(f"Files in GCS (gs://{self.bucket_name}/{prefix}):")
        for blob in blobs:
            # Remove base folder prefix to get relative path
            if self.base_folder:
                relative_path = blob.name.replace(f"{self.base_folder}/", "", 1)
            else:
                relative_path = blob.name
            gcs_files.add(relative_path)
            logger.info(f"  {relative_path}")
        
        # Compare
        missing_in_gcs = local_files - gcs_files
        extra_in_gcs = gcs_files - local_files
        
        if missing_in_gcs:
            logger.error(f"Missing {len(missing_in_gcs)} files in GCS:")
            for file in missing_in_gcs:
                logger.error(f"  - {file}")
            return False
        
        if extra_in_gcs:
            logger.warning(f"Found {len(extra_in_gcs)} extra files in GCS (will remain):")
            for file in extra_in_gcs:
                logger.warning(f"  + {file}")
        
        logger.info("")
        logger.info("✓ Verification successful!")
        return True
    
    def print_summary(self, successful: int, failed: int, start_time: datetime):
        """Print upload summary"""
        duration = (datetime.now() - start_time).total_seconds()
        
        logger.info("")
        logger.info("=" * 60)
        logger.info("Upload Summary")
        logger.info("=" * 60)
        logger.info(f"Total files processed: {successful + failed}")
        logger.info(f"Successful uploads: {successful}")
        logger.info(f"Failed uploads: {failed}")
        logger.info(f"Duration: {duration:.2f} seconds")
        logger.info(f"Destination: gs://{self.bucket_name}/{self.base_folder or '(root)'}")
        logger.info("=" * 60)


def main():
    """Main execution function"""
    start_time = datetime.now()
    
    # Get configuration from environment variables
    bucket_name = os.getenv('GCS_BUCKET_NAME')
    gcp_project = os.getenv('GCP_PROJECT_ID')
    base_folder = os.getenv('GCS_BASE_FOLDER', '')
    source_dir = os.getenv('SOURCE_DIR', '/schemas')
    
    # Validate required parameters
    if not bucket_name:
        logger.error("ERROR: GCS_BUCKET_NAME environment variable is required")
        sys.exit(1)
    
    if not gcp_project:
        logger.error("ERROR: GCP_PROJECT_ID environment variable is required")
        sys.exit(1)
    
    try:
        # Initialize uploader
        uploader = SchemaUploader(bucket_name, gcp_project, base_folder)
        
        # Upload files
        successful, failed = uploader.upload_schemas(source_dir)
        
        if failed > 0:
            logger.error(f"Upload completed with {failed} failures")
            sys.exit(1)
        
        # Verify upload
        if not uploader.verify_upload(source_dir):
            logger.error("Verification failed")
            sys.exit(1)
        
        # Print summary
        uploader.print_summary(successful, failed, start_time)
        
        logger.info("")
        logger.info("✓ Schema upload completed successfully!")
        sys.exit(0)
        
    except Exception as e:
        logger.error(f"Fatal error: {str(e)}", exc_info=True)
        sys.exit(1)


if __name__ == "__main__":
    main()


# =============================================================================
# DOCKERFILE
# File: Dockerfile
# =============================================================================
"""
FROM python:3.11-slim

# Install required packages
RUN pip install --no-cache-dir google-cloud-storage==2.10.0

# Copy schema files
COPY bucket-contents/ /schemas/

# Copy Python script
COPY copy_schemas_to_gcs.py /app/copy_schemas_to_gcs.py

# Set working directory
WORKDIR /app

# Make script executable
RUN chmod +x copy_schemas_to_gcs.py

# Run the upload script
ENTRYPOINT ["python3", "/app/copy_schemas_to_gcs.py"]
"""

# =============================================================================
# REQUIREMENTS.TXT
# File: requirements.txt
# =============================================================================
"""
google-cloud-storage==2.10.0
"""

# =============================================================================
# HELM CHART - Chart.yaml
# File: helm-chart/schema-upload-files/Chart.yaml
# =============================================================================
"""
apiVersion: v2
name: schema-upload-files
description: Kubernetes Job to upload schema files to GCS using Python
type: application
version: 1.0.0
appVersion: "1.0"
keywords:
  - gcs
  - schema
  - upload
  - python
maintainers:
  - name: Platform Team
"""

# =============================================================================
# HELM CHART - values.yaml
# File: helm-chart/schema-upload-files/values.yaml
# =============================================================================
"""
job:
  namePrefix: schema-copy-to-gcs
  ttlSecondsAfterFinished: 3600
  backoffLimit: 3
  resources:
    requests:
      memory: "256Mi"
      cpu: "250m"
    limits:
      memory: "512Mi"
      cpu: "500m"

image:
  repository: ""  # e.g., gcr.io/project/schema-uploader
  tag: ""  # e.g., v1.0.0
  pullPolicy: IfNotPresent

gcs:
  bucketName: ""
  baseFolder: ""
  projectId: ""

serviceAccount:
  name: "gcs-schema-copy-sa"
  annotations:
    iam.gke.io/gcp-service-account: ""

namespace: "default"

labels:
  app: schema-upload
  managed-by: harness

annotations: {}
"""

# =============================================================================
# HELM CHART - serviceaccount.yaml
# File: helm-chart/schema-upload-files/templates/serviceaccount.yaml
# =============================================================================
"""
apiVersion: v1
kind: ServiceAccount
metadata:
  name: {{ .Values.serviceAccount.name }}
  namespace: {{ .Values.namespace }}
  labels:
    {{- range $key, $value := .Values.labels }}
    {{ $key }}: {{ $value }}
    {{- end }}
  {{- with .Values.serviceAccount.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
"""

# =============================================================================
# HELM CHART - job.yaml
# File: helm-chart/schema-upload-files/templates/job.yaml
# =============================================================================
"""
apiVersion: batch/v1
kind: Job
metadata:
  name: {{ .Values.job.namePrefix }}-{{ now | date "20060102-150405" }}
  namespace: {{ .Values.namespace }}
  labels:
    {{- range $key, $value := .Values.labels }}
    {{ $key }}: {{ $value }}
    {{- end }}
    job-type: schema-upload
  {{- with .Values.annotations }}
  annotations:
    {{- toYaml . | nindent 4 }}
  {{- end }}
spec:
  ttlSecondsAfterFinished: {{ .Values.job.ttlSecondsAfterFinished }}
  backoffLimit: {{ .Values.job.backoffLimit }}
  template:
    metadata:
      labels:
        {{- range $key, $value := .Values.labels }}
        {{ $key }}: {{ $value }}
        {{- end }}
        job-type: schema-upload
    spec:
      serviceAccountName: {{ .Values.serviceAccount.name }}
      restartPolicy: Never
      containers:
        - name: schema-uploader
          image: {{ .Values.image.repository }}:{{ .Values.image.tag }}
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          env:
            - name: GCS_BUCKET_NAME
              value: "{{ .Values.gcs.bucketName }}"
            - name: GCP_PROJECT_ID
              value: "{{ .Values.gcs.projectId }}"
            - name: GCS_BASE_FOLDER
              value: "{{ .Values.gcs.baseFolder }}"
            - name: SOURCE_DIR
              value: "/schemas"
          resources:
            {{- toYaml .Values.job.resources | nindent 12 }}
"""

# =============================================================================
# HELM OVERRIDES - bld01-config.yaml
# File: helm-overrides/bld01-config.yaml
# =============================================================================
"""
image:
  repository: "gcr.io/your-project/schema-uploader"
  tag: "latest"
  pullPolicy: Always

gcs:
  bucketName: "your-project-schemas-bld01"
  baseFolder: "schemas"
  projectId: "your-gcp-project-bld01"

serviceAccount:
  name: "gcs-schema-copy-sa"
  annotations:
    iam.gke.io/gcp-service-account: "gcs-schema-copy-sa@your-gcp-project-bld01.iam.gserviceaccount.com"

namespace: "schema-upload"

labels:
  app: schema-upload
  environment: bld01
  managed-by: harness

job:
  namePrefix: schema-copy-to-gcs
  ttlSecondsAfterFinished: 3600
  backoffLimit: 3
  resources:
    requests:
      memory: "256Mi"
      cpu: "250m"
    limits:
      memory: "512Mi"
      cpu: "500m"
"""

# =============================================================================
# HELM OVERRIDES - pre01-config.yaml
# File: helm-overrides/pre01-config.yaml
# =============================================================================
"""
image:
  repository: "gcr.io/your-project/schema-uploader"
  tag: "latest"
  pullPolicy: Always

gcs:
  bucketName: "your-project-schemas-pre01"
  baseFolder: "schemas"
  projectId: "your-gcp-project-pre01"

serviceAccount:
  name: "gcs-schema-copy-sa"
  annotations:
    iam.gke.io/gcp-service-account: "gcs-schema-copy-sa@your-gcp-project-pre01.iam.gserviceaccount.com"

namespace: "schema-upload"

labels:
  app: schema-upload
  environment: pre01
  managed-by: harness

job:
  namePrefix: schema-copy-to-gcs
  ttlSecondsAfterFinished: 7200
  backoffLimit: 2
  resources:
    requests:
      memory: "256Mi"
      cpu: "250m"
    limits:
      memory: "512Mi"
      cpu: "500m"
"""

# =============================================================================
# HELM OVERRIDES - prd01-config.yaml
# File: helm-overrides/prd01-config.yaml
# =============================================================================
"""
image:
  repository: "gcr.io/your-project/schema-uploader"
  tag: "v1.0.0"  # Use specific version tags in production
  pullPolicy: IfNotPresent

gcs:
  bucketName: "your-project-schemas-prd01"
  baseFolder: "schemas"
  projectId: "your-gcp-project-prd01"

serviceAccount:
  name: "gcs-schema-copy-sa"
  annotations:
    iam.gke.io/gcp-service-account: "gcs-schema-copy-sa@your-gcp-project-prd01.iam.gserviceaccount.com"

namespace: "schema-upload"

labels:
  app: schema-upload
  environment: prd01
  managed-by: harness

job:
  namePrefix: schema-copy-to-gcs
  ttlSecondsAfterFinished: 86400
  backoffLimit: 1
  resources:
    requests:
      memory: "512Mi"
      cpu: "500m"
    limits:
      memory: "1Gi"
      cpu: "1000m"
"""

# =============================================================================
# .dockerignore
# File: .dockerignore
# =============================================================================
"""
.git
.gitignore
*.md
helm-chart/
helm-overrides/
.github/
__pycache__/
*.pyc
*.pyo
*.pyd
.Python
venv/
"""

# =============================================================================
# BUILD AND DEPLOYMENT GUIDE
# File: BUILD-DEPLOYMENT-GUIDE.md
# =============================================================================
"""
# Schema Uploader - Build and Deployment Guide

## Repository Structure

```
your-repo/
├── bucket-contents/              # Schema files to upload
│   ├── version1_0/
│   │   └── uapi_schema.json
│   └── version2_0/
│       └── uapi_schema.json
├── copy_schemas_to_gcs.py        # Python upload script
├── requirements.txt              # Python dependencies
├── Dockerfile                    # Docker image definition
├── .dockerignore                 # Docker ignore patterns
├── helm-chart/
│   └── schema-upload-files/
│       ├── Chart.yaml
│       ├── values.yaml
│       └── templates/
│           ├── job.yaml
│           └── serviceaccount.yaml
└── helm-overrides/
    ├── bld01-config.yaml
    ├── pre01-config.yaml
    └── prd01-config.yaml
```

## Local Development and Testing

### 1. Test Python Script Locally
```bash
# Install dependencies
pip install -r requirements.txt

# Set environment variables
export GCS_BUCKET_NAME="your-test-bucket"
export GCP_PROJECT_ID="your-project-id"
export GCS_BASE_FOLDER="test-schemas"
export SOURCE_DIR="./bucket-contents"

# Run the script
python copy_schemas_to_gcs.py
```

### 2. Build Docker Image
```bash
# Build the image
docker build -t schema-uploader:test .

# Test the image locally (requires GCP credentials)
docker run --rm \
  -e GCS_BUCKET_NAME="your-test-bucket" \
  -e GCP_PROJECT_ID="your-project-id" \
  -e GCS_BASE_FOLDER="test-schemas" \
  -v ~/.config/gcloud:/root/.config/gcloud \
  schema-uploader:test
```

## Harness Pipeline Configuration

### Stage 1: Build Docker Image

```yaml
- stage:
    name: Build Schema Uploader Image
    identifier: build_image
    type: CI
    spec:
      cloneCodebase: true
      platform:
        os: Linux
        arch: Amd64
      runtime:
        type: Cloud
        spec: {}
      execution:
        steps:
          - step:
              type: BuildAndPushDockerRegistry
              name: Build and Push Image
              identifier: build_push
              spec:
                connectorRef: <your_docker_registry_connector>
                repo: gcr.io/your-project/schema-uploader
                tags:
                  - <+pipeline.sequenceId>
                  - <+pipeline.variables.image_tag>
                  - latest
                dockerfile: Dockerfile
                context: .
                labels:
                  version: <+pipeline.variables.image_tag>
                  build-number: <+pipeline.sequenceId>
              timeout: 10m
```

### Stage 2: Deploy to Kubernetes

```yaml
- stage:
    name: Deploy Schema Uploader
    identifier: deploy
    type: Deployment
    spec:
      deploymentType: Kubernetes
      service:
        serviceRef: schema_upload_service
        serviceInputs:
          serviceDefinition:
            type: Kubernetes
            spec:
              variables:
                - name: image_tag
                  type: String
                  value: <+pipeline.stages.build_image.spec.execution.steps.build_push.output.outputVariables.tag>
      environment:
        environmentRef: <+input>
        deployToAll: false
        infrastructureDefinitions:
          - identifier: <+input>
      execution:
        steps:
          - step:
              name: Deploy Helm Chart
              identifier: helm_deploy
              type: HelmDeploy
              timeout: 10m
              spec:
                chartPath: helm-chart/schema-upload-files
                helmVersion: V3
                valuesPaths:
                  - helm-overrides/<+env.name>-config.yaml
                commandFlags:
                  - --set image.tag=<+pipeline.stages.build_image.spec.execution.steps.build_push.output.outputVariables.tag>
                skipDryRun: false
          
          - step:
              name: Wait for Job Completion
              identifier: wait_job
              type: ShellScript
              timeout: 30m
              spec:
                shell: Bash
                onDelegate: true
                source:
                  type: Inline
                  spec:
                    script: |
                      #!/bin/bash
                      set -e
                      
                      NAMESPACE="<+infra.namespace>"
                      JOB_LABEL="job-type=schema-upload"
                      
                      echo "Waiting for job completion..."
                      kubectl wait --for=condition=complete \
                        --timeout=30m \
                        -n ${NAMESPACE} \
                        job -l ${JOB_LABEL}
                      
                      echo "Job completed successfully!"
          
          - step:
              name: Get Job Logs
              identifier: get_logs
              type: ShellScript
              timeout: 5m
              spec:
                shell: Bash
                onDelegate: true
                source:
                  type: Inline
                  spec:
                    script: |
                      #!/bin/bash
                      NAMESPACE="<+infra.namespace>"
                      POD_NAME=$(kubectl get pods -n ${NAMESPACE} -l job-type=schema-upload -o jsonpath='{.items[0].metadata.name}')
                      
                      echo "=== Job Logs ==="
                      kubectl logs -n ${NAMESPACE} ${POD_NAME}
        rollbackSteps: []
```

## Manual Deployment Steps

### 1. Build and Push Image
```bash
# Build
docker build -t gcr.io/your-project/schema-uploader:v1.0.0 .

# Push
docker push gcr.io/your-project/schema-uploader:v1.0.0
```

### 2. Deploy with Helm
```bash
# Deploy to BLD01
helm upgrade --install schema-upload \
  ./helm-chart/schema-upload-files \
  -f helm-overrides/bld01-config.yaml \
  --set image.tag=v1.0.0 \
  -n schema-upload \
  --create-namespace

# Check job status
kubectl get jobs -n schema-upload

# View logs
kubectl logs -n schema-upload -l job-type=schema-upload --tail=100 -f
```

### 3. Verify in GCS
```bash
gsutil ls -r gs://your-project-schemas-bld01/schemas/
```

## Python Script Features

✅ **Preserves directory structure** - Maintains exact folder hierarchy  
✅ **Detailed logging** - Shows progress and errors clearly  
✅ **Verification** - Confirms all files uploaded successfully  
✅ **Error handling** - Graceful failure with proper exit codes  
✅ **Workload Identity** - No credentials needed in container  
✅ **Environment-based config** - All settings via env vars  

## Example Output

```
2024-11-13 10:15:30 - INFO - Initialized uploader for bucket: my-schemas-bld01
2024-11-13 10:15:30 - INFO - GCP Project: my-project-bld01
2024-11-13 10:15:30 - INFO - Base folder: schemas
2024-11-13 10:15:30 - INFO - ============================================================
2024-11-13 10:15:30 - INFO - Starting Schema Upload Process
2024-11-13 10:15:30 - INFO - ============================================================
2024-11-13 10:15:30 - INFO - Found 2 files to upload
2024-11-13 10:15:30 - INFO - 
2024-11-13 10:15:30 - INFO - Directory structure:
2024-11-13 10:15:30 - INFO -   version1_0/uapi_schema.json
2024-11-13 10:15:30 - INFO -   version2_0/uapi_schema.json
2024-11-13 10:15:30 - INFO - 
2024-11-13 10:15:31 - INFO - ✓ Uploaded: uapi_schema.json -> gs://my-schemas-bld01/schemas/version1_0/uapi_schema.json
2024-11-13 10:15:31 - INFO - ✓ Uploaded: uapi_schema.json -> gs://my-schemas-bld01/schemas/version2_0/uapi_schema.json
2024-11-13 10:15:31 - INFO - 
2024-11-13 10:15:31 - INFO - ============================================================
2024-11-13 10:15:31 - INFO - Verifying Upload
2024-11-13 10:15:31 - INFO - ============================================================
2024-11-13 10:15:32 - INFO - Files in GCS (gs://my-schemas-bld01/schemas/):
2024-11-13 10:15:32 - INFO -   version1_0/uapi_schema.json
2024-11-13 10:15:32 - INFO -   version2_0/uapi_schema.json
2024-11-13 10:15:32 - INFO - 
2024-11-13 10:15:32 - INFO - ✓ Verification successful!
2024-11-13 10:15:32 - INFO - 
2024-11-13 10:15:32 - INFO - ============================================================
2024-11-13 10:15:32 - INFO - Upload Summary
2024-11-13 10:15:32 - INFO - ============================================================
2024-11-13 10:15:32 - INFO - Total files processed: 2
2024-11-13 10:15:32 - INFO - Successful uploads: 2
2024-11-13 10:15:32 - INFO - Failed uploads: 0
2024-11-13 10:15:32 - INFO - Duration: 2.15 seconds
2024-11-13 10:15:32 - INFO - Destination: gs://my-schemas-bld01/schemas
2024-11-13 10:15:32 - INFO - ============================================================
2024-11-13 10:15:32 - INFO - 
2024-11-13 10:15:32 - INFO - ✓ Schema upload completed successfully!
```

## Troubleshooting

### Check Image Build
```bash
docker run --rm schema-uploader:test python3 --version
docker run --rm schema-uploader:test ls -la /schemas
```

### Test Workload Identity
```bash
kubectl run -it --rm test-wi \
    --image=google/cloud-sdk:alpine \
    --serviceaccount=gcs-schema-copy-sa \
    -n schema-upload \
    -- gsutil ls gs://your-bucket/
```

### Debug Job Failures
```bash
kubectl describe job -n schema-upload -l job-type=schema-upload
kubectl logs -n schema-upload -l job-type=schema-upload
```
"""
